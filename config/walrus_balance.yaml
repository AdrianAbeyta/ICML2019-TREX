walrus: #namespace
    task_and_robot_environment_name: 'WalrusBalance-v0'
    ros_ws_abspath: "/home/jd/trex_ros_ws"
    #running_step: 0.04 # amount of time the control will be executed
    #pos_step: 0.016     # increment in position for each command
    
    # Initial checkpointed PPO2 agent/policy training parameters
    gamma: 0.999
    nepisodes: 4  #Number of episodes to run. 
    nsteps: 10000
    running_step: 0.06 # Time for each step
    log_interval: 1000 #Number of timesteps before logging.
    save_freq: 1000 #Frequency of model checkpoints that will be saved during training.
    total_timesteps: 100000 
    checkpoint_dir: walrus_balance/checkpoints # Relative path to save checkpointed models

    # Preference model parameters
    pref_model_dir: walrus_balance/pref_model/gt # Relative path to save the preference model (reward function)
    min_length: 100 # Minimum trajectory length for suboptimal trajectories to train pref model
    include_action: true # Whether or not to include action in the reward approximation function
    num_models: 5 # Number of preference models to train
    steps: 50 # Length of trajectory data snippet used to train the Preference Model
    num_layers: 2 # Number of layers for the fully-connected neural network (reward function approximation)
    embedding_dims: 256 # Number of nodes in the hidden NN layers
    D: 1000 # Number of data snippets to train the preference model
    l2_reg: .01 # L2 regularization size (loss function?)
    noise: .1 # Noise level to add onto training label
turtlebot3: #namespace
    
    #Logging and enviornment parameters

    task_and_robot_environment_name: 'TurtleBot3World-v0' #The robot env and task env id. Edit to change agent world env
    ros_ws_abspath: "/home/jd/trex_ros_ws" #Workspace containing your openai_ros and turtlbot3 ROS files.
    pkg_path: "ICML2019-TREX" #Identifies the path in which to reference the necessary parameters for launch. 
    save_path: '/home/jd/trex_ros_ws/src/learner/ppo2_checkpoints'
    running_step: 0.04 # amount of time the control will be executed.
    pos_step: 0.016 # Increment in position for each command.
    

    #PPO2 Checkpoint Parameters

    gamma: 0.999
    nepisodes: 8 #Number of episodes to run. 
    nsteps: 10000 #Number of steps to run for each episode. 
    running_step: 0.06 # Time for each step.
    log_interval: 1000 #Number of timesteps before logging.
    save_freq: 1000 #Frequency of model checkpoints that will be saved durring training.
    total_timesteps: 10000 #Number of TIME steps aloud for each episode.
    

    #trex reward parameters
    checkpoint_dir: learner/ppo2_checkpoints/turtlebot3
    pref_model_dir: learner/pref_model/trex_reward_turtlebot3
    min_length: 100 # Minimum trajectory length for suboptimal trajectories to train pref model
    include_action: true # Whether or not to include action in the reward approximation function
    num_models: 5 # Number of preference models to train
    steps: 50 # Length of trajectory data snippet used to train the Preference Model
    num_layers: 2 # Number of layers for the fully-connected neural network (reward function approximation)
    embedding_dims: 256 # Number of nodes in the hidden NN layers
    D: 1000 # Number of data snippets to train the preference model
    l2_reg: .01 # L2 regularization size (loss function?)
    noise: .1 # Noise level to add onto training label